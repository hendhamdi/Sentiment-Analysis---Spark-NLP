import matplotlib.pyplot as plt
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, when, rand
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark import SparkConf, SparkContext
from prettytable import PrettyTable

# Configuration Spark
conf = SparkConf().setAppName("SentimentAnalysisMP2L").setMaster("local[*]") \
    .set("spark.executor.memory", "4g") \
    .set("spark.driver.memory", "4g")
sc = SparkContext(conf=conf)

# SparkSession
spark = SparkSession.builder.appName("SentimentAnalysisMP2L").getOrCreate()

# Charger les donn√©es
df = spark.read.csv("data/avis_etudiants_dataset.csv", header=True, inferSchema=True)
df = df.select("Text", "Mati√®re", "Semestre").na.drop()

# Cr√©er la colonne Sentiment
df = df.withColumn("Sentiment",
    when(lower(col("Text")).rlike(".*(pas|nul|difficile|compliqu√©|mauvais|incompr√©hensible).*"), "negative")
    .when(lower(col("Text")).rlike(".*(bon|utile|clair|int√©ressant|super|excellent|parfait|facile|bien|g√©nial).*"), "positive")
    .otherwise("neutral")
)

# R√©duction des neutres ‚Üí plus de positifs
df = df.withColumn("Sentiment", 
    when((col("Sentiment") == "neutral") & (rand() < 0.9), "positive")  # 90% des neutres deviennent positifs
    .otherwise(col("Sentiment"))
)

# Indexation du label
indexer = StringIndexer(inputCol="Sentiment", outputCol="label")

# Pipeline NLP
tokenizer = Tokenizer(inputCol="Text", outputCol="words")
remover = StopWordsRemover(inputCol="words", outputCol="filtered")
hashingTF = HashingTF(inputCol="filtered", outputCol="rawFeatures", numFeatures=10000)
idf = IDF(inputCol="rawFeatures", outputCol="features")

# Mod√®le
lr = LogisticRegression(maxIter=10, regParam=0.001)
pipeline = Pipeline(stages=[indexer, tokenizer, remover, hashingTF, idf, lr])

# Split
training, test = df.randomSplit([0.7, 0.3])

# Entra√Ænement
model = pipeline.fit(training)

# Pr√©dictions
predictions = model.transform(test)

# √âvaluation
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)

# Table d'exemples
table = PrettyTable()
table.field_names = ["üìù Avis", "üéØ R√©el", "ü§ñ Pr√©dit"]

# R√©cup√©rer le mapping des √©tiquettes
labels_mapping = indexer.fit(df).labels
labels_dict = {i: label for i, label in enumerate(labels_mapping)}

rows = predictions.select("Text", "Sentiment", "prediction").take(15)
for row in rows:
    texte = row.Text[:60].replace("\n", " ") + "..."
    sentiment = row.Sentiment
    prediction = labels_dict[int(row.prediction)] if int(row.prediction) in labels_dict else "?"
    table.add_row([texte, sentiment, prediction])

print("\nüìä Exemples de pr√©dictions :")
print(table)

# Analyse globale
sentiment_counts = predictions.groupBy("Sentiment").count().collect()
sentiment_map = {"negative": 0, "neutral": 0, "positive": 0}
for row in sentiment_counts:
    sentiment_map[row["Sentiment"]] = row["count"]

# Graphique
labels = ["N√©gatif", "Neutre", "Positif"]
values = [sentiment_map["negative"], sentiment_map["neutral"], sentiment_map["positive"]]

plt.figure(figsize=(8, 5))
plt.bar(labels, values, color=["#001f3f", "#0074D9", "#00BFFF"])  # Bleu fonc√© ‚Üí clair
plt.title("R√©partition des sentiments")
plt.xlabel("Cat√©gorie")
plt.ylabel("Nombre d'avis")
plt.savefig('output/results.png')
plt.show()

# R√©sum√©
print(f"\n‚úÖ Analyse termin√©e")
print(f"- Donn√©es d'entra√Ænement : {training.count()} exemples")
print(f"- Donn√©es de test : {test.count()} exemples")
print(f"- Pr√©cision du mod√®le : {accuracy:.2f}")

# Fonction pour interface web
def get_analysis_results():
    return {
        "accuracy": accuracy,
        "counts": sentiment_map,
        "examples": rows
    }

if __name__ == "__main__":
    results = get_analysis_results()
    print(f"Analyse termin√©e avec pr√©cision: {results['accuracy']:.2f}")
    spark.stop()
